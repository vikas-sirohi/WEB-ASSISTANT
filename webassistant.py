# -*- coding: utf-8 -*-
"""WebAssistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n1h2XSshfT95IUySxq3cqCfBcBOkXZeq
"""

!pip install langgraph
!pip install langchain_groq
!pip install wikipedia
!pip install langchain
!pip install langchain_community

from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.document_loaders import WikipediaLoader
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.utilities.tavily_search import TavilySearchAPIWrapper

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from IPython.display import Image, display

from typing_extensions import TypedDict
import operator
from typing import Annotated


# LLM
from google.colab import userdata
GROQ_API_KEY = userdata.get('GROQ_API_KEY')
from langchain_groq import ChatGroq

llm = ChatGroq(model = "mixtral-8x7b-32768",
        temperature = 0,
        max_tokens = 100,
        max_retries = 2,
        api_key = GROQ_API_KEY
        )

#==============AGENT==================

# Tavily API Confi.
from google.colab import userdata
TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')

tavilySearchAPIWrapper = TavilySearchAPIWrapper(tavily_api_key=TAVILY_API_KEY)

# StateSchema
class State(TypedDict):
  question: str
  answer: str
  context: Annotated[list, operator.add]

def search_web(state: State):
  """ Retrieve docs from the Web Search."""
  #Search
  tavily_search = TavilySearchResults(max_results = 2,  api_wrapper=tavilySearchAPIWrapper)
  result = tavily_search.invoke(state['question'])
  formatted_docs = [
      f"Documents {doc['content']}" for doc in result
  ]

  return {"context": [formatted_docs]}

def search_wikipedia(state: State):
  """Retrieve docs from wikipedia."""

  #Search
  question = state["question"]
  search_docs = WikipediaLoader(query = question, load_max_docs=2).load()

  formatted_search = [
      f"Documents: {doc.page_content}" for doc in search_docs
  ]
  return {"context": [formatted_search]}

def generated_answer(state: State):

  """Node to answer a question."""

  # Get State:
  question = state["question"]
  context = state["context"]

  # Template
  answer_template = """Answer the question {question} using this context: {context} """
  answer_instruction = answer_template.format(question = question,
                                              context = context)

  # Answer
  answer = llm.invoke([SystemMessage(content = "You are a helpful agent.")]  + [HumanMessage(content = answer_instruction)])

  return {"answer": answer}

builder = StateGraph(State)

builder.add_node("search_web", search_web)
builder.add_node("search_wikipedia", search_wikipedia)
builder.add_node("generate_answer", generated_answer)

builder.add_edge(START, "search_wikipedia")
builder.add_edge(START, "search_web")
builder.add_edge("search_wikipedia", "generate_answer")
builder.add_edge("search_web", "generate_answer")
builder.add_edge("generate_answer", END)

memory = MemorySaver()
graph = builder.compile(checkpointer = memory)


thread = {"configurable": {"thread_id": "1"}} #Memory

print("Please Enter 'exit' to stop conversation.")
#Chat Loop
while True:
  query = input("You:: ")
  if query.lower() == 'exit':
    break
  result = graph.invoke({"question": query}, thread)
  answer = result["answer"].content
  print(f"AI: {answer}")


# An Example
#result = graph.invoke({"question": "Who is Gigori Perelman, What problem did he solved?"}, thread)
#display(Image(graph.get_graph().draw_mermaid_png())) [View the Graph]

display(Image(graph.get_graph().draw_mermaid_png()))